{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %pip install -e .\n",
    "\n",
    "HERE = Path.cwd().resolve()\n",
    "REPO = HERE.parent\n",
    "SRC  = REPO / \"src\"\n",
    "\n",
    "# make 'boreas' importable from source tree\n",
    "if (SRC / \"boreas\").exists():\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "from boreas import ModelParams, MassLoss, Fractionation\n",
    "from boreas.flow_solutions import FlowSolutions as FS\n",
    "\n",
    "try:\n",
    "    from plots import Plots # if plots/ has an init.py file\n",
    "except ImportError:\n",
    "    from plots.plots import Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader for M-R .ddat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDataLoader:\n",
    "    def __init__(self, base_path: Path, params: ModelParams):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.params = params\n",
    "\n",
    "    def load_single_ddat_file(self, ddat_filename):\n",
    "        \"\"\"\n",
    "        Load a single .ddat file by name and return arrays for mass, radius, and temperature.\n",
    "        \"\"\"\n",
    "        p = self.base_path / ddat_filename\n",
    "        \n",
    "        rows = []\n",
    "        \n",
    "        with p.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if not s or s.startswith(\"#\"):\n",
    "                    continue\n",
    "                row = [float(x) for x in s.split()]\n",
    "                if row[1] == -1.0:\n",
    "                    continue\n",
    "                rows.append(row)\n",
    "                \n",
    "        if not rows:\n",
    "            raise ValueError(f\"No valid rows in {p}\")\n",
    "\n",
    "        data = np.array(rows, dtype=float)\n",
    "        mass_me     = data[:, 0]\n",
    "        radius_re   = data[:, 1]\n",
    "        teq_k       = data[:, 2]\n",
    "        mass_g      = mass_me  * self.params.mearth\n",
    "        radius_c    = radius_re* self.params.rearth\n",
    "        return mass_g, radius_c, teq_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grids and composition: H₂ + H₂O only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params  = ModelParams() # do this before setting the X_*, otherwise they will be reset\n",
    "\n",
    "base    = Path(\"/Users/mvalatsou/PhD/Repos/MR_perplex/OUTPUT/paper\") # path of M-R .ddat files\n",
    "out_dir = Path(\"/Users/mvalatsou/PhD/Repos/MR_perplex/OUTPUT/paper_rerun/XUV_filter\") # where to save the re-run\n",
    "\n",
    "params.set_composition({\"H2\": 0., \"H2O\": 1.}, auto_normalize=True)\n",
    "ddat_file = \"3H2O_superEarth.ddat\"\n",
    "csv_path = HERE / \"T-FXUV.csv\" # Pierlou data\n",
    "\n",
    "mass_loss     = MassLoss(params)\n",
    "fractionation = Fractionation(params)\n",
    "loader        = ModelDataLoader(base, params)\n",
    "\n",
    "mass, radius, Teq = loader.load_single_ddat_file(ddat_file)\n",
    "print(mass.shape, radius.shape, Teq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FXUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fxuv  = pd.read_csv(csv_path)\n",
    "\n",
    "if \"teq\" not in df_fxuv.columns or \"fxuv\" not in df_fxuv.columns:\n",
    "    raise ValueError(\"CSV must have columns 'Teq' and 'FXUV'.\")\n",
    "    \n",
    "col_T = \"teq\"\n",
    "col_F = \"fxuv\"\n",
    "if col_T not in df_fxuv.columns or col_F not in df_fxuv.columns:\n",
    "    # try some common variants\n",
    "    ren = {}\n",
    "    for c in df_fxuv.columns:\n",
    "        lc = c.strip().lower()\n",
    "        if lc in (\"teq\",\"teq_k\",\"temperature\",\"eq_temp\"): ren[c] = col_T\n",
    "        if lc in (\"fxuv\",\"f_xuv\",\"xuv_flux\"):             ren[c] = col_F\n",
    "    df_fxuv = df_fxuv.rename(columns=ren)\n",
    "    assert col_T in df_fxuv.columns and col_F in df_fxuv.columns, \\\n",
    "        f\"CSV must have '{col_T}' and '{col_F}' columns.\"\n",
    "\n",
    "# Optional: de-duplicate within each Teq while preserving order\n",
    "def unique_in_order(x): \n",
    "    seen=set(); out=[]\n",
    "    for v in x:\n",
    "        if v not in seen: seen.add(v); out.append(v)\n",
    "    return out\n",
    "\n",
    "# Handle floating Teq binning: round to e.g. nearest 1 K to stabilize grouping\n",
    "df_fxuv[\"Teq_key\"] = df_fxuv[col_T].round(0)\n",
    "\n",
    "# Build: Teq_key -> np.array([FXUV_1, FXUV_2, ...])\n",
    "fxuv_map = (\n",
    "    df_fxuv\n",
    "    .groupby(\"Teq_key\", sort=True)[col_F]\n",
    "    .apply(lambda s: np.array(unique_in_order(list(s.astype(float))), dtype=float))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Helper: get FXUVs for a Teq (exact rounded match; fallback to nearest key)\n",
    "def fxuvs_for_teq(teq_val: float, keys=None):\n",
    "    if keys is None: keys = np.array(sorted(fxuv_map.keys()), dtype=float)\n",
    "    k = float(np.round(teq_val, 0))\n",
    "    if k in fxuv_map:\n",
    "        return fxuv_map[k]\n",
    "    # fallback to nearest available Teq\n",
    "    nearest = float(keys[np.argmin(np.abs(keys - k))])\n",
    "    return fxuv_map[nearest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model grouped by unique Teq (reusing FXUVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "DIAG_COLS = [\"EL_min_abs_f\", \"EL_Rbest\", \"EL_Rbest_over_Rp\", \"skip_reason\"]\n",
    "COMP_KEYS = [\"H2\",\"H2O\",\"O2\",\"CO2\",\"CO\",\"CH4\",\"N2\",\"NH3\",\"H2S\",\"SO2\",\"S2\"]\n",
    "def composition_payload(p):\n",
    "    return {f\"X_{k}\": float(getattr(p, f\"X_{k}\")) for k in COMP_KEYS}\n",
    "\n",
    "for T in np.unique(Teq):\n",
    "    print(f\"\\n Running Teq = {T} K\")\n",
    "    mask = np.isclose(Teq, T)\n",
    "    M, R, Tgrp = mass[mask], radius[mask], Teq[mask]\n",
    "    \n",
    "    for FXUV in fxuvs_for_teq(float(T)): # <-- values from T-FXUV.csv (Pierlou data)\n",
    "        print(f\"Running FXUV = {FXUV} erg/cm^2/s\")\n",
    "        params.update_param(\"FXUV\", float(FXUV))\n",
    "        \n",
    "        ml  = mass_loss.compute_mass_loss_parameters(M, R, Tgrp)\n",
    "        ml_ok = [rec for rec in ml if rec.get(\"regime\") != \"SKIPPED\"]\n",
    "        ml_skipped = [rec for rec in ml if rec.get(\"regime\") == \"SKIPPED\"]\n",
    "        \n",
    "        fr  = fractionation.execute(ml_ok, mass_loss, allow_dynamic_light_major=True, forced_light_major=\"H\", tol=1e-5, max_iter=100)\n",
    "        \n",
    "        comp = composition_payload(params)\n",
    "        \n",
    "        for rec in fr:\n",
    "            row = {\"FXUV\": float(FXUV)}\n",
    "            row.update(rec)    # <- bring in ALL keys from mass-loss + fractionation\n",
    "            row.update(comp)   # <- composition snapshot\n",
    "            for c in DIAG_COLS:\n",
    "                row.setdefault(c, float(\"nan\"))\n",
    "            rows.append(row)\n",
    "\n",
    "            \n",
    "        # also record skipped with diagnostics\n",
    "        for rec in ml_skipped:\n",
    "            row = {\"FXUV\": float(FXUV)}\n",
    "            row.update(rec)\n",
    "            row.update(comp)\n",
    "            for c in DIAG_COLS:\n",
    "                row.setdefault(c, rec.get(c, float(\"nan\")))\n",
    "            rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "# ensure all columns present even if missing in some rows\n",
    "all_cols = sorted(set().union(*(r.keys() for r in rows)))\n",
    "df_results = df_results.reindex(columns=all_cols)\n",
    "\n",
    "csv_name = Path(ddat_file).with_suffix(\".csv\").name\n",
    "csv_path = out_dir / csv_name\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(\"wrote\", csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
